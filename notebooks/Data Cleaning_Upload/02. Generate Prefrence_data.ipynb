{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c0b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "from datasets import Dataset\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4371204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceSet:\n",
    "    def __init__(self, triples: List[Tuple[str, str, str]]):\n",
    "        self.triples = triples\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'PreferenceSet':\n",
    "        data = json.loads(json_str)\n",
    "        triples = [(triple['instruction'], triple['generated_answer'], triple['extracted_answer']) for triple in data['preference_triples']]\n",
    "        return cls(triples)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c30b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_from_json(file_path: str) -> Dataset:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "        # \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n",
    "        # \"content\": [item[\"content\"] for item in data[\"artifact_data\"]],\n",
    "        # \"platform\": [item[\"platform\"] for item in data[\"artifact_data\"]],\n",
    "        # \"author_id\": [item[\"author_id\"] for item in data[\"artifact_data\"]],\n",
    "        # \"author_full_name\": [item[\"author_full_name\"] for item in data[\"artifact_data\"]],\n",
    "        # \"link\": [item[\"link\"] for item in data[\"artifact_data\"]],\n",
    "        \"id\": [dat['_id'] for dat in data],\n",
    "        \"content\": [dat['content'] for dat in data],\n",
    "        \"platform\": [dat['platform'] for dat in data],\n",
    "        \"author_id\": [dat['author_id'] for dat in data],\n",
    "        \"author_full_name\": [dat['author_full_name'] for dat in data],\n",
    "        \"link\": [dat['link'] for dat in data],\n",
    "\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee311b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47419369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -> List[str]:\n",
    "    extracts = []\n",
    "    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\"\n",
    "    # print(dataset)\n",
    "    for article in dataset[\"content\"]:\n",
    "        cleaned_article = clean_text(article['Content'])\n",
    "        sentences = re.split(sentence_pattern, cleaned_article)\n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            if len(current_chunk) + len(sentence) <= max_length:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                if len(current_chunk) >= min_length:\n",
    "                    extracts.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "        if len(current_chunk) >= min_length:\n",
    "            extracts.append(current_chunk.strip())\n",
    "    return extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c27843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_triples(extract: str, client: OpenAI) -> List[Tuple[str, str, str]]:\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following extract, generate five instruction-answer triples. Each triple should consist of:\n",
    "        1. An instruction asking about a specific topic in the context.\n",
    "        2. A generated answer that attempts to answer the instruction based on the context.\n",
    "        3. An extracted answer that is a relevant excerpt directly from the given context.\n",
    "    \n",
    "    Instructions must be self-contained and general, without explicitly mentioning a context, system, course, or extract.\n",
    "    \n",
    "    Important:\n",
    "    - Ensure that the extracted answer is a verbatim copy from the context, including all punctuation and apostrophes.\n",
    "    - Do not add any ellipsis (...) or [...]  to indicate skipped text in the extracted answer.\n",
    "    - If the relevant text is not continuous, use two separate sentences from the context instead of skipping text.\n",
    "    \n",
    "    Provide your response in JSON format with the following structure:\n",
    "    {{\n",
    "        \"preference_triples\": [\n",
    "            {{\n",
    "                \"instruction\": \"...\",\n",
    "                \"generated_answer\": \"...\",\n",
    "                \"extracted_answer\": \"...\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Extract:\n",
    "    {extract}\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(model=\"gpt-4o-mini\", messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant who generates instruction-answer triples based on the given context. \"\n",
    "            \"           Each triple should include an instruction, a generated answer, and an extracted answer from the context. Provide your response in JSON format.\",\n",
    "            \n",
    "        },\n",
    "            \n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "         },\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    result = PreferenceSet.from_json(completion.choices[0].message.content)\n",
    "    \n",
    "    return result.triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12dcdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_answers(dataset: Dataset, min_length: int = 100) -> Dataset:\n",
    "    def is_long_enough(example):\n",
    "        return len(example['chosen']) >= min_length\n",
    "    \n",
    "    return dataset.filter(is_long_enough)\n",
    "\n",
    "def filter_answer_format(dataset: Dataset) -> Dataset:\n",
    "    def is_valid_format(example):\n",
    "        chosen = example['chosen']\n",
    "        return (len(chosen) > 0 and\n",
    "                chosen[0].isupper() and\n",
    "            chosen[-1] in ('.', '!', '?'))\n",
    "    \n",
    "    return dataset.filter(is_valid_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e03c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -> Dataset:\n",
    "    extracts = extract_substrings(dataset)\n",
    "    preference_triples = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(generate_preference_triples, extract, client)\n",
    "            for extract in extracts\n",
    "        ]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "           preference_triples.extend(future.result())\n",
    "           instructions, generated_answers, extracted_answers = zip(*preference_triples)\n",
    "    \n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"prompt\": list(instructions),\n",
    "            \"rejected\": list(generated_answers),\n",
    "            \"chosen\": list(extracted_answers)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01a10d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\Aquib\\AppData\\Local\\Temp\\ipykernel_15044\\2389218043.py:5: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  raw_dataset = load_articles_from_json(\"D:\\Project\\MirrorMuse\\data\\data_warehouse_raw_data\\ArticleDocument.json\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset:\n",
      "                                      id  \\\n",
      "0   01ce4881-1ec9-40d4-85c8-c4626d302094   \n",
      "1   9d2718e9-f3d7-4d89-9ee8-3d11bca693bc   \n",
      "2   f2877873-ed13-434d-9b6c-a8f50f47bdc8   \n",
      "3   ceb1afa3-dc45-4bb3-afc0-08286827b08f   \n",
      "4   3e3e824a-6442-49f7-b26b-b7a538c24335   \n",
      "..                                   ...   \n",
      "71  ef25f83c-dd53-416e-87f1-29c54ce3f4f4   \n",
      "72  6c0c601d-17ac-4cdf-afc1-740eb7355826   \n",
      "73  f73f6263-9456-4d31-be47-b5ae3ac6dfa6   \n",
      "74  22958dc9-c43a-4992-839f-a82e8be0fcda   \n",
      "75  396a112a-8bbd-48fe-b2f9-9402016bdc8d   \n",
      "\n",
      "                                              content  \\\n",
      "0   {'Content': 'Maxime Labonne\n",
      "\n",
      "  * __LLM Course\n",
      "...   \n",
      "1   {'Content': 'Maxime Labonne\n",
      "\n",
      "  * __LLM Course\n",
      "...   \n",
      "2   {'Content': '# Maxime Labonne\n",
      "\n",
      "SubscribeSign i...   \n",
      "3   {'Content': '# Maxime Labonne\n",
      "\n",
      "SubscribeSign i...   \n",
      "4   {'Content': '# Maxime Labonne\n",
      "\n",
      "SubscribeSign i...   \n",
      "..                                                ...   \n",
      "71  {'Content': '#\n",
      "\n",
      "SubscribeSign in\n",
      "\n",
      "#### Share t...   \n",
      "72  {'Content': '#\n",
      "\n",
      "SubscribeSign in\n",
      "\n",
      "#### Share t...   \n",
      "73  {'Content': '#\n",
      "\n",
      "SubscribeSign in\n",
      "\n",
      "#### Share t...   \n",
      "74  {'Content': '#\n",
      "\n",
      "SubscribeSign in\n",
      "\n",
      "#### Share t...   \n",
      "75  {'Content': '#\n",
      "\n",
      "SubscribeSign in\n",
      "\n",
      "#### Share t...   \n",
      "\n",
      "                      platform                             author_id  \\\n",
      "0           mlabonne.github.io  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "1           mlabonne.github.io  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "2   maximelabonne.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "3   maximelabonne.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "4   maximelabonne.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "..                         ...                                   ...   \n",
      "71     decodingml.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "72     decodingml.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "73     decodingml.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "74     decodingml.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "75     decodingml.substack.com  baed4873-defd-4899-be69-2e27fa3a7a12   \n",
      "\n",
      "   author_full_name                                               link  \n",
      "0    Aquib Ali Khan  https://mlabonne.github.io/blog/posts/2024-07-...  \n",
      "1    Aquib Ali Khan  https://mlabonne.github.io/blog/posts/2024-07-...  \n",
      "2    Aquib Ali Khan  https://maximelabonne.substack.com/p/uncensor-...  \n",
      "3    Aquib Ali Khan  https://maximelabonne.substack.com/p/create-mi...  \n",
      "4    Aquib Ali Khan  https://maximelabonne.substack.com/p/merge-lar...  \n",
      "..              ...                                                ...  \n",
      "71   Aquib Ali Khan  https://decodingml.substack.com/p/dml-chain-of...  \n",
      "72   Aquib Ali Khan  https://decodingml.substack.com/p/dml-build-an...  \n",
      "73   Aquib Ali Khan  https://decodingml.substack.com/p/dml-4-key-id...  \n",
      "74   Aquib Ali Khan  https://decodingml.substack.com/p/dml-how-to-a...  \n",
      "75   Aquib Ali Khan  https://decodingml.substack.com/p/dml-top-6-ml...  \n",
      "\n",
      "[76 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# def main(dataset_id: str) -> Dataset:\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Load the raw data\n",
    "raw_dataset = load_articles_from_json(\"D:\\Project\\MirrorMuse\\data\\data_warehouse_raw_data\\ArticleDocument.json\")\n",
    "print(\"Raw dataset:\")\n",
    "print(raw_dataset.to_pandas())\n",
    "    \n",
    "    # return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f9f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b2ed4daa064e76864d414ea237422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference dataset:\n",
      "                                                 prompt  \\\n",
      "0     What is recommended for new domains unknown to...   \n",
      "1     What are the three popular SFT techniques ment...   \n",
      "2                   What does full fine tuning involve?   \n",
      "3           How does LoRA differ from full fine tuning?   \n",
      "4     What advantage does QLoRA provide over standar...   \n",
      "...                                                 ...   \n",
      "2105  What tools does the author primarily use for p...   \n",
      "2106  What is the process the author follows to hand...   \n",
      "2107  What does the author refer to as 'The Warehouse'?   \n",
      "2108            What free course does the author offer?   \n",
      "2109  How does the author feel about using too many ...   \n",
      "\n",
      "                                               rejected  \\\n",
      "0     It is recommended to continuously pre-train th...   \n",
      "1     The three popular SFT techniques are full fine...   \n",
      "2     Full fine tuning involves retraining all param...   \n",
      "3     LoRA freezes the weights and introduces small ...   \n",
      "4     QLoRA offers up to 33 additional memory reduct...   \n",
      "...                                                 ...   \n",
      "2105  The author primarily uses Brave, Notion, and G...   \n",
      "2106  The author collects, plans, distills, and stor...   \n",
      "2107  The Warehouse is where the author takes distil...   \n",
      "2108  The author offers a free course called 'The Fu...   \n",
      "2109  The author believes that you don't need 100 to...   \n",
      "\n",
      "                                                 chosen  \n",
      "0     For new domains unknown to the base model, it ...  \n",
      "1     The three most popular SFT techniques are full...  \n",
      "2     Full fine tuning is the most straightforward S...  \n",
      "3     Low Rank Adaptation LoRA is a popular paramete...  \n",
      "4     QLoRA Quantization aware Low Rank Adaptation i...  \n",
      "...                                                 ...  \n",
      "2105  I primarily use only Brave, Notion, and Google...  \n",
      "2106       You have to collect link plan distill store.  \n",
      "2107  Here is where I take the distilled information...  \n",
      "2108  The Full Stack 7 Steps MLOps Framework a 7 les...  \n",
      "2109  You don't need 100 tools to be productive. The...  \n",
      "\n",
      "[2110 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Create preference dataset\n",
    "dataset = create_preference_dataset(raw_dataset, client)\n",
    "print(\"Preference dataset:\")\n",
    "print(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f133786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a4f0797f3542169fe97b23efe3c80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Filter out samples with short answers\n",
    "dataset = filter_short_answers(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03c73e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f895b6440f44cf48e8834fb7e843b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Filter answers based on format\n",
    "dataset = filter_answer_format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0a59c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211ec56559f74bd2b0c082ef9cfa6927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb4460a722147b0a7663571fad174f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/SkillRipper/preference-data/commit/33a1c0e199906dc1cdeb38f7392477949ae6facc', commit_message='Upload dataset', commit_description='', oid='33a1c0e199906dc1cdeb38f7392477949ae6facc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/SkillRipper/preference-data', endpoint='https://huggingface.co', repo_type='dataset', repo_id='SkillRipper/preference-data'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Export\n",
    "dataset.push_to_hub(\"SkillRipper/preference-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4728a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirrormuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
